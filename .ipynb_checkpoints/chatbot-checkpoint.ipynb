{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c96258-a4a3-49e9-bd63-1348d9ee9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56752f86-a98e-4510-afa3-bf34c51a2d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\UTENTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perceptron_targger: Package\n",
      "[nltk_data]     'averaged_perceptron_targger' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\UTENTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\UTENTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\UTENTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "2025-04-18 16:26:28.196 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\UTENTE\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-04-18 16:26:28.199 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "# Télechargement des ressources nécessaires \n",
    "\n",
    "nltk.download('punkt_tab')  #Nécessaire pour découper un texte en phrase et en mots (tokenisation)\n",
    "nltk.download('averaged_perceptron_targger') #C'est nécessaire pour reconnaitre la nature des mots(nom, verbe, adjectif)=étiquetage grammatical\n",
    "nltk.download('stopwords') #liste des mots courants inutiles pour l'analyse(exemple: le, la, les,...), à suprimer du texte\n",
    "nltk.download('wordnet')  #dictionnaire lexical pour faire de la lemmatisation(trouver la forme de base des mots)\n",
    "nltk.download('omw-1.4')  #nécessaire pour que WordNet puisse fonctioné correctement\n",
    "\n",
    "import streamlit as st\n",
    "import string   #bibliothéque utilisé pour les opérations(sur les chaînes)\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize #word_tokenize(permet de découpouper les phrases en mots),sent_tokenize(pour découper en phrase)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer # WordNetLemmatizer permet de faire de la lemmatisation de mot àà sa forme de base\n",
    "\n",
    "#Chargemment du texte (le texte est en français)\n",
    "with open('pub_entreprise.txt' ,'r', encoding = 'utf-8') as f:\n",
    "    data = f.read().replace('\\n', '')\n",
    "    \n",
    "# Découper (on va diviser le texte en phrases)\n",
    "sentences = sent_tokenize(data)\n",
    "stop_words = set(stopwords.words('french'))\n",
    "#Prétraitement: tokenisation, nettoyer, leme=matisation\n",
    "\n",
    "#Nettoyage d'une phrase en supprimant les mots inutiles et en ramenant les mots à leuurs forme de base\n",
    "def preprocess(sentence):\n",
    "    \n",
    "    words = word_tokenize(sentence, language ='french')  #Découpe la phrase en mots en précisant que le texte est en français\n",
    "    words = [word.lower() for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "    #word.lower: transforme tous les mots en minuscules (pour éviter les doublons)\n",
    "    #if word.lower() not in stop_word: on enléve les mots vides(comme les articles : le, la, est, et...)\n",
    "    #and word not in string.punctuation : on enléve aussi la ponctuation(comme :,!,?, etc)\n",
    "\n",
    "    #on crée un lemmantiseur/ ramner les mots à leur forme de base\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    #On applique le lemmatiseur à chaque mot dans la liste\n",
    "    words = [lemmatizer.lemmatize(word) for word in words] \n",
    "\n",
    "    #La fonction retourne la liste des mots final nettoyés et simplifiés.\n",
    "    return words \n",
    "\n",
    "#Corpus prétraité : c'est une collection de textes ou de phrases, souvent utilisée en traitement de texte\n",
    "corpus = [preprocess(sentence) for sentence in sentences]\n",
    "\n",
    "#Fonction de recherche de la phrase la plus pertinente\n",
    "#Cette fonction cherche la phrase la plus pertinente du texte par rapport à une question posée par l'utilisateur(appelée ici query)\n",
    "\n",
    "def get_most_relevant_sentence(query):  #obtenir la phrase la plus pertinente\n",
    "    query = preprocess(query) #query est le paramétre de la fonction. Il représente la question ou la requête de l'utulisateur\n",
    "    #On utilise la fonction preprocess pour traiter la requête de l'utilisateur\n",
    "\n",
    "    #on initialise une variable max_similarity à 0\n",
    "    #Cette variable va stocker le score de similarité maximal trouvé jusqu'à présent\n",
    "    #Au début, on n'a pas encore comparé à aucune phrase, donc la similarité maximale est égal à zéro\n",
    "    max_similarity = 0\n",
    "\n",
    "    #on initialise une variable most_relevant_sentence à une chaîne de caractére vide\n",
    "    #cette variable va stocker la phrase du corpus qui a la plus grande similarité avec la requête\n",
    "    #Au début on n'a pas triuver de phrase pertinente, donc elle est vide\n",
    "    most_relevant_sentence = \"\"\n",
    "\n",
    "    #C'est une boucle for qui va parcourir toutes les phrases du corpus\n",
    "    #énumerate(corpus) permet d'obtenir à la fois(i) et la phrase(sentence)\n",
    "    for i, sentence in enumerate(corpus):\n",
    "        similarity = len(set(query).intersection(sentence))/float(len(set(query).union(sentence)))\n",
    "        \n",
    "        #set(query):converti la requête en un ensemble de mots uniques.\n",
    "        #set(sentence):converti la phrase courante en un ensemble de mots uniques.\n",
    "        #set(query).intersection(sentence):Trouve les mots qui sont à la fois dans la requête et dans la phrase.\n",
    "        #len(set(query).intersection(sentence)):compte le nombre de mots communs.\n",
    "        #set(query).union(sentence):trouve tous les mots qui sont soit dans la requête, soit dans la phrase.\n",
    "        #len(set(query).union(sentence)):compte le nombre total de mots uniques dans la requete et dans la phrase.\n",
    "        #la similarité est calculée comme le nombre de mots communs divisé par le nombre total de mots uniques. Cela donne une mesure de la proportion de mots partagés entre la requête et la phrase\n",
    "\n",
    "        #on vérifie si la situation calculée pour la phrase courant est supérieur à la similarité maximale trouvée\n",
    "        if similarity > max_similarity :\n",
    "            max_similarity = similarity\n",
    "            most_relevant_sentence = sentence[i]\n",
    "    return most_relevant_sentence\n",
    "\n",
    "#Interface streamlit \n",
    "\n",
    "st.title(\"Chatbot Eclat Brillant\")\n",
    "question = st.text_input(\"Posez une question sur nos produits :\")\n",
    "\n",
    "if question:\n",
    "    response = get_most_relevant_sentence(question)\n",
    "    st.write(\"Reponse : \",response)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a752650-6b63-4702-a405-853897ed9b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4dbdf7-f4d4-4d50-83da-41141f315427",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af3275-3b01-4c98-a3c2-f6f6ec78da0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a10a69-ffb5-44b8-a210-43b6c75236b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
